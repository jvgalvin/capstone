{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "441ca1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johngalvin/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pydicom\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from PIL import Image\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f5d3fa",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "1. Fine-tune VGG16 on OASIS dataset \n",
    "2. Use fine-tuned VGG16 to extract features from ADNI images\n",
    "3. Pool the feature vectors for each patient into a single vector\n",
    "4. Concatenate the pooled vector with the clinical/genetic data\n",
    "5. Train model\n",
    "\n",
    "# What is in this notebook?\n",
    "1. Fine-tuned VGG16 model\n",
    "2. Neural network for ADNI prediction (our task)\n",
    "3. SVC for ADNI prediction (our task)\n",
    "4. Image-only neural network for ADNI prediction (our task)\n",
    "\n",
    "Note that all 4 models in this notebook are trained on data that was passed through the VGG16 feature extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33da297",
   "metadata": {},
   "source": [
    "## Fine-tune VGG16 on OASIS dataset\n",
    "\n",
    "Note: We must retrain a portion of the trunk since we will remove the head we attach here to generate embeddings later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4e557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 1\n",
    "IMG_SIZE = (224,224) # Expected size for VGG16\n",
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f37c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and convert images to .jpeg\n",
    "img_dir = \"/Users/johngalvin/Desktop/OASIS/0/\"\n",
    "\n",
    "for file in os.listdir(img_dir):\n",
    "    if file.endswith(\".JPG\") or file.endswith(\".jpg\"):\n",
    "        img = Image.open(img_dir + file)\n",
    "        file_name, file_ext = os.path.splitext(file)\n",
    "        new_name = file_name + \".jpeg\"\n",
    "        img.save(img_dir + new_name)\n",
    "        \n",
    "img_dir = \"/Users/johngalvin/Desktop/OASIS/1/\"\n",
    "\n",
    "for file in os.listdir(img_dir):\n",
    "    if file.endswith(\".JPG\") or file.endswith(\".jpg\"):\n",
    "        img = Image.open(img_dir + file)\n",
    "        file_name, file_ext = os.path.splitext(file)\n",
    "        new_name = file_name + \".jpeg\"\n",
    "        img.save(img_dir + new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a14f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the .JPG and.jpg files\n",
    "img_dir = \"/Users/johngalvin/Desktop/OASIS/0/\"\n",
    "for file in os.listdir(img_dir):\n",
    "    if file.endswith(\".JPG\") or file.endswith(\".jpg\"):\n",
    "        path_to_file = os.path.join(\"/Users/johngalvin/Desktop/OASIS/0/\", file)\n",
    "        os.remove(path_to_file)\n",
    "        \n",
    "img_dir = \"/Users/johngalvin/Desktop/OASIS/1/\"\n",
    "for file in os.listdir(img_dir):\n",
    "    if file.endswith(\".JPG\") or file.endswith(\".jpg\"):\n",
    "        path_to_file = os.path.join(\"/Users/johngalvin/Desktop/OASIS/1/\", file)\n",
    "        os.remove(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2585cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "arrays = []\n",
    "\n",
    "img_dir = \"/Users/johngalvin/Desktop/OASIS/0/\"\n",
    "for file in os.listdir(img_dir):\n",
    "    fpath = os.path.join(\"/Users/johngalvin/Desktop/OASIS/0/\", file)\n",
    "    img = Image.open(fpath).convert(\"L\")  # Convert the image to grayscale\n",
    "    resized_image = img.resize((224, 224), Image.BILINEAR)  # Resize the image\n",
    "    resized_array = np.expand_dims(np.array(resized_image, dtype=np.uint8), axis=-1)  # Convert to NumPy array and add channel dimension\n",
    "    targets.append(0)\n",
    "    arrays.append(resized_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f05161",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"/Users/johngalvin/Desktop/OASIS/1/\"\n",
    "for file in os.listdir(img_dir):\n",
    "    fpath = os.path.join(\"/Users/johngalvin/Desktop/OASIS/1/\", file)\n",
    "    img = Image.open(fpath).convert(\"L\")  # Convert the image to grayscale\n",
    "    resized_image = img.resize((224, 224), Image.BILINEAR)  # Resize the image\n",
    "    resized_array = np.expand_dims(np.array(resized_image, dtype=np.uint8), axis=-1)  # Convert to NumPy array and add channel dimension\n",
    "    targets.append(1)\n",
    "    arrays.append(resized_array)\n",
    "    \n",
    "X = np.array(arrays)\n",
    "y = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a223b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance positive and negative class\n",
    "y = y[-y.sum()*2:]\n",
    "X = X[-y.sum()*2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e94fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_set, X_test, y_train_set, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reduce size to fit in memory\n",
    "X_train = X_train_set[:6000]\n",
    "y_train = y_train_set[:6000]\n",
    "X_val = X_train_set[6000:7000]\n",
    "y_val = y_train_set[6000:7000]\n",
    "X_test = X_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "num_samples, height, width, channels = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(num_samples, -1)\n",
    "X_train_scaled_2d = scaler.fit_transform(X_train_reshaped)\n",
    "X_train_scaled = X_train_scaled_2d.reshape(num_samples, height, width, channels)\n",
    "\n",
    "num_samples, height, width, channels = X_val.shape\n",
    "X_val_reshaped = X_val.reshape(num_samples, -1)\n",
    "X_val_scaled_2d = scaler.transform(X_val_reshaped)\n",
    "X_val_scaled = X_val_scaled_2d.reshape(num_samples, height, width, channels)\n",
    "\n",
    "num_samples, height, width, channels = X_test.shape\n",
    "X_test_reshaped = X_test.reshape(num_samples, -1)\n",
    "X_test_scaled_2d = scaler.transform(X_test_reshaped)\n",
    "X_test_scaled = X_test_scaled_2d.reshape(num_samples, height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1f2207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add channels (VGG16 expects 3 channels)\n",
    "X_train_rgb = np.repeat(X_train_scaled, 3, axis=-1)\n",
    "X_val_rgb = np.repeat(X_val_scaled, 3, axis=-1)\n",
    "X_test_rgb = np.repeat(X_test_scaled, 3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b4a6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model():\n",
    "    \n",
    "    base_model = VGG16(include_top=False,\n",
    "                       input_shape= IMG_SIZE + (3,),\n",
    "                       weights=\"imagenet\")\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=IMG_SIZE + (3,))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(NUM_CLASSES)(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84ceab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 13:07:10.945549: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-09 13:07:10.945618: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Pro\n"
     ]
    }
   ],
   "source": [
    "base = build_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0897878a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,108,929\n",
      "Trainable params: 394,241\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbc304e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johngalvin/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 13:08:20.667055: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 135s 1s/step - loss: 0.7013 - accuracy: 0.6078 - val_loss: 0.5591 - val_accuracy: 0.7510\n",
      "Epoch 2/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4827 - accuracy: 0.7482 - val_loss: 0.4372 - val_accuracy: 0.7840\n",
      "Epoch 3/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4431 - accuracy: 0.7807 - val_loss: 0.5232 - val_accuracy: 0.6980\n",
      "Epoch 4/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4280 - accuracy: 0.7872 - val_loss: 0.4101 - val_accuracy: 0.7880\n",
      "Epoch 5/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4171 - accuracy: 0.7898 - val_loss: 0.4977 - val_accuracy: 0.6870\n",
      "Epoch 6/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4011 - accuracy: 0.8032 - val_loss: 0.3795 - val_accuracy: 0.8130\n",
      "Epoch 7/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4326 - accuracy: 0.7785 - val_loss: 0.4021 - val_accuracy: 0.8200\n",
      "Epoch 8/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4042 - accuracy: 0.8090 - val_loss: 0.3864 - val_accuracy: 0.7990\n",
      "Epoch 9/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4021 - accuracy: 0.8115 - val_loss: 0.4211 - val_accuracy: 0.7640\n",
      "Epoch 10/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4057 - accuracy: 0.8085 - val_loss: 0.4437 - val_accuracy: 0.7170\n",
      "Epoch 11/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4185 - accuracy: 0.7920 - val_loss: 0.3740 - val_accuracy: 0.8310\n",
      "Epoch 12/15\n",
      "94/94 [==============================] - 131s 1s/step - loss: 0.4001 - accuracy: 0.7955 - val_loss: 0.3737 - val_accuracy: 0.8050\n",
      "Epoch 13/15\n",
      "94/94 [==============================] - 131s 1s/step - loss: 0.3971 - accuracy: 0.7960 - val_loss: 0.4352 - val_accuracy: 0.7660\n",
      "Epoch 14/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.3797 - accuracy: 0.8058 - val_loss: 0.4141 - val_accuracy: 0.8300\n",
      "Epoch 15/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4089 - accuracy: 0.7965 - val_loss: 0.3835 - val_accuracy: 0.7900\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "base_history = base.fit(X_train_rgb,\n",
    "                        y_train,\n",
    "                        validation_data=[X_val_rgb, y_val],\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d03216bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94/94 [==============================] - 147s 2s/step - loss: 0.3910 - accuracy: 0.8053 - val_loss: 0.3005 - val_accuracy: 0.8830\n",
      "Epoch 2/10\n",
      "94/94 [==============================] - 146s 2s/step - loss: 0.2805 - accuracy: 0.8708 - val_loss: 0.2324 - val_accuracy: 0.9120\n",
      "Epoch 3/10\n",
      "94/94 [==============================] - 146s 2s/step - loss: 0.2449 - accuracy: 0.8822 - val_loss: 0.1881 - val_accuracy: 0.9290\n",
      "Epoch 4/10\n",
      "94/94 [==============================] - 146s 2s/step - loss: 0.1801 - accuracy: 0.9165 - val_loss: 0.1854 - val_accuracy: 0.9200\n",
      "Epoch 5/10\n",
      "94/94 [==============================] - 144s 2s/step - loss: 0.1450 - accuracy: 0.9248 - val_loss: 0.1555 - val_accuracy: 0.9450\n",
      "Epoch 6/10\n",
      "94/94 [==============================] - 143s 2s/step - loss: 0.1135 - accuracy: 0.9337 - val_loss: 0.1337 - val_accuracy: 0.9390\n",
      "Epoch 7/10\n",
      "94/94 [==============================] - 144s 2s/step - loss: 0.0761 - accuracy: 0.9692 - val_loss: 0.1206 - val_accuracy: 0.9620\n",
      "Epoch 8/10\n",
      "94/94 [==============================] - 143s 2s/step - loss: 0.0558 - accuracy: 0.9822 - val_loss: 0.1525 - val_accuracy: 0.9390\n",
      "Epoch 9/10\n",
      "94/94 [==============================] - 143s 2s/step - loss: 0.0236 - accuracy: 0.9948 - val_loss: 0.0920 - val_accuracy: 0.9710\n",
      "Epoch 10/10\n",
      "94/94 [==============================] - 143s 2s/step - loss: 0.0123 - accuracy: 0.9972 - val_loss: 0.0865 - val_accuracy: 0.9640\n"
     ]
    }
   ],
   "source": [
    "base.trainable = True\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                  patience=3,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "base.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "base_history_2 = base.fit(X_train_rgb,\n",
    "                          y_train,\n",
    "                          validation_data=[X_val_rgb, y_val],\n",
    "                          epochs=10,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d67ee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 6s 187ms/step - loss: 0.1406 - accuracy: 0.9560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14064964652061462, 0.956000030040741]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.evaluate(X_test_rgb, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "babc6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = tf.keras.Model(base.input, base.get_layer(\"global_average_pooling2d\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0098bcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f6f803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: feature_extractor/feature_extractor_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: feature_extractor/feature_extractor_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p feature_extractor\n",
    "feature_extractor.save(\"feature_extractor/feature_extractor_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf785df",
   "metadata": {},
   "source": [
    "## Generate embeddings for ADNI images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d78cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0473/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0473/MPRAGE/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/032_S_0677/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/032_S_0677/MPRAGE/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/032_S_0677/MPRAGE/2016-07-22_09_23_31.0/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0210/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0210/MPRAGE/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0210/MPRAGE/2016-11-03_14_21_45.0/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n"
     ]
    }
   ],
   "source": [
    "pt_ids = []\n",
    "pixels = []\n",
    "\n",
    "directory_path = '/Users/johngalvin/Downloads/ADNI 2'\n",
    "\n",
    "# Iterate through level 2 subdirectories\n",
    "for level_2_foldername in os.listdir(directory_path):\n",
    "    level_2_folder_path = os.path.join(directory_path, level_2_foldername)\n",
    "    \n",
    "    if os.path.isdir(level_2_folder_path):\n",
    "        # Iterate through DICOM files in level 5 (bottom-most level) of each level 2 folder\n",
    "        for root, _, files in os.walk(level_2_folder_path):\n",
    "            for file in files:\n",
    "                try:\n",
    "                    file_path = os.path.abspath(os.path.join(root, file))\n",
    "                    \n",
    "                    # Attempt to read DICOM file\n",
    "                    dcm = pydicom.dcmread(file_path)\n",
    "                    \n",
    "                    # Check if the file has PixelData (to avoid non-image DICOM files)\n",
    "                    if hasattr(dcm, 'PixelData'):\n",
    "                        # Append both level 2 folder name and pixel array to the lists\n",
    "                        pt_ids.append(file[5:15])\n",
    "                        pixels.append(dcm.pixel_array)\n",
    "                except Exception as e:\n",
    "                    # Handle exceptions (e.g., files without 'TransferSyntaxUID')\n",
    "                    print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "mri_df = pd.DataFrame({'PTID': pt_ids, 'Pixel Array': pixels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83119559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize image arrays with Bilinear Interpolation\n",
    "resized_arrays = []\n",
    "\n",
    "for val in mri_df[\"Pixel Array\"]:\n",
    "    image = Image.fromarray(val, mode='L')\n",
    "    resized_image = image.resize((224, 224), Image.BILINEAR)\n",
    "    resized_array = np.expand_dims(np.array(resized_image, dtype=np.uint8), axis=-1) #TF expects channel dim\n",
    "    resized_arrays.append(resized_array)\n",
    "    \n",
    "mri_df[\"Pixel Array\"] = resized_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22814860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep just the first 160 images for each patient (size/speed)\n",
    "mri_df = mri_df.groupby(\"PTID\").head(160)\n",
    "mri_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883549c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 3 channels for VGG16 (num_samples, 224, 224, 3) - after running this cell\n",
    "resized_arrays = []\n",
    "\n",
    "for i in range(len(mri_df[\"Pixel Array\"])):\n",
    "    resized_arrays.append(np.repeat(mri_df[\"Pixel Array\"][i], 3, axis=-1))\n",
    "    \n",
    "mri_df[\"Pixel Array\"] = resized_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37dfb361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in med/famhist\n",
    "mf_hist = pd.read_csv('../data/clinical_training_data_with_medhist_famhist.csv')\n",
    "\n",
    "# Handle Nan\n",
    "mf_hist[\"Family_History_of_AD\"] = mf_hist[\"Family_History_of_AD\"].fillna(0)\n",
    "mf_hist[\"Family_History_of_Dementia\"] = mf_hist[\"Family_History_of_Dementia\"].fillna(0)\n",
    "\n",
    "# For converting categorical variables to ints\n",
    "label_encoder = LabelEncoder()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Split features / target\n",
    "X = mf_hist.drop(columns=['AD_dx_in_5_yrs'])\n",
    "y = mf_hist['AD_dx_in_5_yrs']\n",
    "\n",
    "# Encode features\n",
    "X[\"Diagnosis_at_Baseline\"] = label_encoder.fit_transform(X[\"Diagnosis_at_Baseline\"])\n",
    "X[\"Gender\"] = label_encoder.fit_transform(X[\"Gender\"])\n",
    "X[\"Ethnicity\"] = label_encoder.fit_transform(X[\"Ethnicity\"])\n",
    "X[\"Race\"] = label_encoder.fit_transform(X[\"Race\"])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5205c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale pixel data\n",
    "train_unscaled_mri_df = pd.merge(mri_df, X_train[[\"PTID\"]], on=\"PTID\", how=\"inner\")\n",
    "test_unscaled_mri_df = pd.merge(mri_df, X_test[[\"PTID\"]], on=\"PTID\", how=\"inner\")\n",
    "\n",
    "train_unscaled_arrays = np.array(train_unscaled_mri_df[\"Pixel Array\"].tolist())\n",
    "test_unscaled_arrays = np.array(test_unscaled_mri_df[\"Pixel Array\"].tolist())\n",
    "\n",
    "mean = np.mean(train_unscaled_arrays, axis=(0, 1, 2))\n",
    "std = np.std(train_unscaled_arrays, axis=(0, 1, 2))\n",
    "\n",
    "train_scaled_array = (train_unscaled_arrays - mean) / std\n",
    "test_scaled_array = (test_unscaled_arrays - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2830793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder labels (order changed when forming unscaled_mri_df)\n",
    "y_train_final = []\n",
    "for val in train_unscaled_mri_df[\"PTID\"].value_counts().index.values:\n",
    "    y_train_final.append(mf_hist.loc[mf_hist[\"PTID\"] == val, \"AD_dx_in_5_yrs\"].values[0])\n",
    "y_train_final = np.array(y_train_final)\n",
    "\n",
    "y_test_final = []\n",
    "for val in test_unscaled_mri_df[\"PTID\"].value_counts().index.values:\n",
    "    y_test_final.append(mf_hist.loc[mf_hist[\"PTID\"] == val, \"AD_dx_in_5_yrs\"].values[0])\n",
    "y_test_final = np.array(y_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97cb6e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [23.54082765 23.54082765 23.54082765]\n",
      "STD: [31.03262227 31.03262227 31.03262227]\n"
     ]
    }
   ],
   "source": [
    "print (f'Mean: {mean}')\n",
    "print (f'STD: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7014d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 09:50:49.627817: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-11 09:50:49.627855: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "feature_extractor = tf.keras.models.load_model(\"feature_extractor/feature_extractor_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287400f",
   "metadata": {},
   "source": [
    "## Generate embeddings and pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80c49b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings and pool\n",
    "patient_ids = []\n",
    "latent_reps = []\n",
    "\n",
    "start = 0\n",
    "end = 160\n",
    "for i in range(len(train_unscaled_mri_df[\"PTID\"].value_counts().index.values)):\n",
    "    p_id = train_unscaled_mri_df[\"PTID\"].value_counts().index.values[i]\n",
    "    patient_ids.append(p_id)\n",
    "    features = feature_extractor(train_scaled_array[start:end]).numpy() # Each patient has 160 images (160, 512)\n",
    "    latent_rep = np.mean(np.stack(features, axis=-1), axis=1) # (512,), max pooling\n",
    "    latent_reps.append(latent_rep)\n",
    "    start += 160\n",
    "    end += 160\n",
    "\n",
    "train_embeddings = pd.DataFrame()\n",
    "train_embeddings[\"PTID\"] = patient_ids\n",
    "train_embeddings[\"embedding\"] = latent_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e01ab22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings and pool\n",
    "patient_ids = []\n",
    "latent_reps = []\n",
    "\n",
    "start = 0\n",
    "end = 160\n",
    "for i in range(len(test_unscaled_mri_df[\"PTID\"].value_counts().index.values)):\n",
    "    p_id = test_unscaled_mri_df[\"PTID\"].value_counts().index.values[i]\n",
    "    patient_ids.append(p_id)\n",
    "    features = feature_extractor(test_scaled_array[start:end]).numpy() # Each patient has 160 images (160, 512)\n",
    "    latent_rep = np.mean(np.stack(features, axis=-1), axis=1) # (512,), max pooling\n",
    "    latent_reps.append(latent_rep)\n",
    "    start += 160\n",
    "    end += 160\n",
    "\n",
    "test_embeddings = pd.DataFrame()\n",
    "test_embeddings[\"PTID\"] = patient_ids\n",
    "test_embeddings[\"embedding\"] = latent_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f9e90bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join back into X_train, X_test\n",
    "X_train = pd.merge(train_embeddings, X_train, on=\"PTID\")\n",
    "X_test = pd.merge(test_embeddings, X_test, on=\"PTID\")\n",
    "\n",
    "# Scale remaining data (clinical/genetic)\n",
    "embedding_column_train = X_train[\"embedding\"]\n",
    "embedding_column_test = X_test[\"embedding\"]\n",
    "\n",
    "columns_to_scale_train = X_train.drop(columns=[\"embedding\", \"PTID\"])\n",
    "columns_to_scale_test = X_test.drop(columns=[\"embedding\", \"PTID\"])\n",
    "\n",
    "scaled_train = scaler.fit_transform(columns_to_scale_train)\n",
    "scaled_test = scaler.transform(columns_to_scale_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(data=scaled_train, columns=columns_to_scale_train.columns)\n",
    "X_test_scaled = pd.DataFrame(data=scaled_test, columns=columns_to_scale_test.columns)\n",
    "\n",
    "X_train_scaled[\"embedding\"] = embedding_column_train\n",
    "X_test_scaled[\"embedding\"] = embedding_column_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f6e8b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train_scaled, y_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e8f9a",
   "metadata": {},
   "source": [
    "## Concatenate pooled embedding with clinical/genetic feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad7f6359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure for training\n",
    "vector_column = X_train_resampled[\"embedding\"].values\n",
    "other_columns = X_train_resampled.drop(columns=[\"embedding\"]).values\n",
    "X_train_final = np.hstack((other_columns, np.vstack(vector_column)))\n",
    "\n",
    "# Structure for test\n",
    "vector_column = X_test_scaled[\"embedding\"].values\n",
    "other_columns = X_test_scaled.drop(columns=[\"embedding\"]).values\n",
    "X_test_final = np.hstack((other_columns, np.vstack(vector_column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7491ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_FEATURES = X_train_final[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f9880011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dropout=0.3, learning_rate=0.0001, l2_penalty=0.1):\n",
    "    \"\"\"Builds classification model\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    inputs = tf.keras.layers.Input(shape=(IN_FEATURES,), name=\"input_layer\") # (Batch, num_features)\n",
    "    \n",
    "    hidden_1 = tf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_penalty), name=\"hidden_1\")(inputs)\n",
    "    hidden_1 = tf.keras.layers.Dropout(dropout)(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_penalty), name=\"hidden_2\")(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dropout(dropout)(hidden_2)\n",
    "    \n",
    "    classification = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"classification_layer\")(hidden_1)\n",
    "    classification_model = tf.keras.Model(inputs=[inputs], outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "                                 metrics=\"accuracy\")\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4bb04648",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "05da5fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 34.2275 - accuracy: 0.4912\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 31.7622 - accuracy: 0.5294\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 29.4582 - accuracy: 0.5706\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 27.3424 - accuracy: 0.5382\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 25.3486 - accuracy: 0.5971\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 23.5101 - accuracy: 0.6029\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 21.7950 - accuracy: 0.5735\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 20.1980 - accuracy: 0.6029\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 18.7125 - accuracy: 0.5853\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 17.3212 - accuracy: 0.6500\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 16.0322 - accuracy: 0.6735\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 14.8366 - accuracy: 0.6559\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 13.7253 - accuracy: 0.6941\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 12.6949 - accuracy: 0.6824\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 22ms/step - loss: 11.7349 - accuracy: 0.7088\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 10.8434 - accuracy: 0.7147\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 10.0234 - accuracy: 0.7206\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 9.2477 - accuracy: 0.7441\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 8.5489 - accuracy: 0.7206\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 7.8962 - accuracy: 0.7176\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 7.2821 - accuracy: 0.7882\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 6.7252 - accuracy: 0.7471\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 6.2162 - accuracy: 0.7588\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 5.7335 - accuracy: 0.7412\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 5.2925 - accuracy: 0.7500\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 4.8799 - accuracy: 0.7824\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 4.5097 - accuracy: 0.7618\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 4.1580 - accuracy: 0.7794\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 3.8560 - accuracy: 0.7529\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 3.5636 - accuracy: 0.7794\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 3.2956 - accuracy: 0.7735\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 3.0499 - accuracy: 0.7765\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 2.8253 - accuracy: 0.7794\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 2.6239 - accuracy: 0.7647\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 2.4398 - accuracy: 0.7794\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 2.2783 - accuracy: 0.7559\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 2.1131 - accuracy: 0.7471\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 1.9644 - accuracy: 0.7794\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 1.8381 - accuracy: 0.7618\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 1.7136 - accuracy: 0.7882\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.6141 - accuracy: 0.7824\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.5177 - accuracy: 0.7588\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.4255 - accuracy: 0.8029\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 1.3438 - accuracy: 0.7765\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.2675 - accuracy: 0.7618\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.2060 - accuracy: 0.8000\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.1488 - accuracy: 0.7647\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.0983 - accuracy: 0.7706\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.0452 - accuracy: 0.7824\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.0007 - accuracy: 0.7676\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.9643 - accuracy: 0.7794\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.9283 - accuracy: 0.7735\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.8982 - accuracy: 0.7735\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.8725 - accuracy: 0.8000\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.8410 - accuracy: 0.8118\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.8268 - accuracy: 0.7794\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.7966 - accuracy: 0.8000\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.7807 - accuracy: 0.7912\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.7635 - accuracy: 0.7882\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.7490 - accuracy: 0.8059\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.7335 - accuracy: 0.7824\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.7227 - accuracy: 0.8059\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.7107 - accuracy: 0.8000\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.7068 - accuracy: 0.8000\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6925 - accuracy: 0.7941\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6838 - accuracy: 0.8059\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6804 - accuracy: 0.7824\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6697 - accuracy: 0.7912\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6607 - accuracy: 0.8265\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6652 - accuracy: 0.8000\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6558 - accuracy: 0.7971\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6473 - accuracy: 0.7971\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6518 - accuracy: 0.7941\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6402 - accuracy: 0.8088\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6404 - accuracy: 0.8147\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6304 - accuracy: 0.8118\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6292 - accuracy: 0.8059\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6315 - accuracy: 0.7912\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6256 - accuracy: 0.8118\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.6244 - accuracy: 0.8000\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.6209 - accuracy: 0.8147\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6189 - accuracy: 0.8000\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6252 - accuracy: 0.7941\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6119 - accuracy: 0.8176\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6161 - accuracy: 0.8059\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6204 - accuracy: 0.7971\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6118 - accuracy: 0.8176\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6117 - accuracy: 0.7941\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6074 - accuracy: 0.8059\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.5997 - accuracy: 0.8118\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6079 - accuracy: 0.7941\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6057 - accuracy: 0.7853\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6012 - accuracy: 0.8118\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6022 - accuracy: 0.8029\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.5955 - accuracy: 0.8147\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.5999 - accuracy: 0.8088\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.5971 - accuracy: 0.8176\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.5942 - accuracy: 0.8059\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.5969 - accuracy: 0.8000\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.5922 - accuracy: 0.8147\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "history = model.fit(X_train_final,\n",
    "                    y_train_resampled,\n",
    "                    #validation_data=[X_test_final, y_test_final],\n",
    "                    batch_size=16,\n",
    "                    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5cb5faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6153 - accuracy: 0.7460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6152703762054443, 0.7460317611694336]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_final, y_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "030323ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If simply guessing all 0, can get this accuracy: 0.746031746031746\n"
     ]
    }
   ],
   "source": [
    "print (\"If simply guessing all 0, can get this accuracy:\", 1 - y_test_final.sum() / y_test_final.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7743615",
   "metadata": {},
   "source": [
    "# Try SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6151833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a319cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5aa5c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10],                \n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\", 0.1],\n",
    "    \"degree\": [1, 2, 3, 4, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6210afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "990f874a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10], 'degree': [1, 2, 3, 4, 5],\n",
       "                         'gamma': ['scale', 'auto', 0.1],\n",
       "                         'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train_final, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97f5390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "Best Accuracy: 0.8941176470588236\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters and best accuracy score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8aeefe0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, degree=2, gamma=0.1, kernel='poly')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate and fit model\n",
    "svc = SVC(C=10, degree=2, gamma=0.1, kernel=\"poly\")\n",
    "\n",
    "svc.fit(X_train_final, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9194cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83        47\n",
      "           1       0.50      0.44      0.47        16\n",
      "\n",
      "    accuracy                           0.75        63\n",
      "   macro avg       0.66      0.64      0.65        63\n",
      "weighted avg       0.74      0.75      0.74        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assess\n",
    "y_pred = svc.predict(X_test_final)\n",
    "\n",
    "# evaluate predictions\n",
    "print(classification_report(y_test_final, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02515fd5",
   "metadata": {},
   "source": [
    "# What if we do image-only?\n",
    "\n",
    "Note: this model below accepts the pooled embeddings formed after VGG16 feature extraction. The only difference from above is that it does **not** concatenate each final pooled embedding with the clinical/genetic feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a794380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_img_only = np.vstack(X_train_resampled[\"embedding\"])\n",
    "y_train_img_only = y_train_resampled\n",
    "\n",
    "X_test_img_only = np.vstack(X_test_scaled[\"embedding\"])\n",
    "y_test_img_only = y_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0d634de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dropout=0.3, learning_rate=0.0001, l2_penalty=0.01):\n",
    "    \"\"\"Builds classification model\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    inputs = tf.keras.layers.Input(shape=(512,), name=\"input_layer\") # (Batch, num_features)\n",
    "    \n",
    "    hidden_1 = tf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_penalty), name=\"hidden_1\")(inputs)\n",
    "    hidden_1 = tf.keras.layers.Dropout(dropout)(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_penalty), name=\"hidden_2\")(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dropout(dropout)(hidden_2)\n",
    "    \n",
    "    classification = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"classification_layer\")(hidden_2)\n",
    "    classification_model = tf.keras.Model(inputs=[inputs], outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "                                 metrics=\"accuracy\")\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7823ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_only_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "92af560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 5.7025 - accuracy: 0.5471 - val_loss: 5.5228 - val_accuracy: 0.4603\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 5.3979 - accuracy: 0.5441 - val_loss: 5.2710 - val_accuracy: 0.4603\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 5.1655 - accuracy: 0.5559 - val_loss: 5.0291 - val_accuracy: 0.4603\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 4.9138 - accuracy: 0.5235 - val_loss: 4.8019 - val_accuracy: 0.4286\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 4.6748 - accuracy: 0.5294 - val_loss: 4.5682 - val_accuracy: 0.4603\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 4.4424 - accuracy: 0.5412 - val_loss: 4.3384 - val_accuracy: 0.4444\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 4.2495 - accuracy: 0.5353 - val_loss: 4.1683 - val_accuracy: 0.4286\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 4.0629 - accuracy: 0.5235 - val_loss: 3.9470 - val_accuracy: 0.5238\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 3.8796 - accuracy: 0.5382 - val_loss: 3.7879 - val_accuracy: 0.4603\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 3.7145 - accuracy: 0.5765 - val_loss: 3.6279 - val_accuracy: 0.4762\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 3.5502 - accuracy: 0.5853 - val_loss: 3.4737 - val_accuracy: 0.4921\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 3.4002 - accuracy: 0.5618 - val_loss: 3.3102 - val_accuracy: 0.4762\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 3.2554 - accuracy: 0.5206 - val_loss: 3.1879 - val_accuracy: 0.4921\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 3.1263 - accuracy: 0.5382 - val_loss: 3.0625 - val_accuracy: 0.4921\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 2.9973 - accuracy: 0.5471 - val_loss: 2.9419 - val_accuracy: 0.4921\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 2.8728 - accuracy: 0.5529 - val_loss: 2.8112 - val_accuracy: 0.4921\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 2.7653 - accuracy: 0.5618 - val_loss: 2.7223 - val_accuracy: 0.4603\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 2.6545 - accuracy: 0.5706 - val_loss: 2.6202 - val_accuracy: 0.4603\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 2.5607 - accuracy: 0.5765 - val_loss: 2.5124 - val_accuracy: 0.4762\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 2.4709 - accuracy: 0.5471 - val_loss: 2.4362 - val_accuracy: 0.4603\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 2.3786 - accuracy: 0.5706 - val_loss: 2.3672 - val_accuracy: 0.3968\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 2.2984 - accuracy: 0.5647 - val_loss: 2.2625 - val_accuracy: 0.4762\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 2.2220 - accuracy: 0.5588 - val_loss: 2.2019 - val_accuracy: 0.4444\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 2.1527 - accuracy: 0.5471 - val_loss: 2.1175 - val_accuracy: 0.4921\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 2.0786 - accuracy: 0.5588 - val_loss: 2.0510 - val_accuracy: 0.4603\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 2.0196 - accuracy: 0.5647 - val_loss: 1.9883 - val_accuracy: 0.4603\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.9610 - accuracy: 0.5706 - val_loss: 1.9398 - val_accuracy: 0.4762\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.8982 - accuracy: 0.5853 - val_loss: 1.8796 - val_accuracy: 0.4762\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.8473 - accuracy: 0.5324 - val_loss: 1.8532 - val_accuracy: 0.4127\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.7987 - accuracy: 0.5765 - val_loss: 1.7845 - val_accuracy: 0.4762\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.7530 - accuracy: 0.5559 - val_loss: 1.7423 - val_accuracy: 0.4444\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.7047 - accuracy: 0.5765 - val_loss: 1.7099 - val_accuracy: 0.4286\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.6570 - accuracy: 0.5735 - val_loss: 1.6360 - val_accuracy: 0.4762\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.6377 - accuracy: 0.5588 - val_loss: 1.6330 - val_accuracy: 0.4127\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.5803 - accuracy: 0.5824 - val_loss: 1.5630 - val_accuracy: 0.4762\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.5525 - accuracy: 0.5853 - val_loss: 1.5514 - val_accuracy: 0.4444\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 1.5163 - accuracy: 0.5824 - val_loss: 1.5076 - val_accuracy: 0.4603\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.4915 - accuracy: 0.5941 - val_loss: 1.4836 - val_accuracy: 0.4921\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.4534 - accuracy: 0.5824 - val_loss: 1.4712 - val_accuracy: 0.4286\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.4341 - accuracy: 0.5559 - val_loss: 1.4284 - val_accuracy: 0.4762\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.4059 - accuracy: 0.5676 - val_loss: 1.3956 - val_accuracy: 0.4603\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.3849 - accuracy: 0.5676 - val_loss: 1.3737 - val_accuracy: 0.4444\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.3612 - accuracy: 0.5735 - val_loss: 1.3501 - val_accuracy: 0.4444\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.3339 - accuracy: 0.5971 - val_loss: 1.3468 - val_accuracy: 0.4286\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.3204 - accuracy: 0.5618 - val_loss: 1.3124 - val_accuracy: 0.4444\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.2968 - accuracy: 0.5647 - val_loss: 1.2944 - val_accuracy: 0.4444\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.2768 - accuracy: 0.5735 - val_loss: 1.2920 - val_accuracy: 0.4286\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.2585 - accuracy: 0.5971 - val_loss: 1.2558 - val_accuracy: 0.4444\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.2417 - accuracy: 0.6265 - val_loss: 1.2316 - val_accuracy: 0.5079\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.2204 - accuracy: 0.6088 - val_loss: 1.2247 - val_accuracy: 0.4603\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.2260 - accuracy: 0.5441 - val_loss: 1.2079 - val_accuracy: 0.4921\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.2006 - accuracy: 0.5912 - val_loss: 1.2047 - val_accuracy: 0.4603\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.1872 - accuracy: 0.5853 - val_loss: 1.1901 - val_accuracy: 0.4603\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.1839 - accuracy: 0.5353 - val_loss: 1.1506 - val_accuracy: 0.5714\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.1582 - accuracy: 0.5853 - val_loss: 1.1581 - val_accuracy: 0.4921\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.1552 - accuracy: 0.5765 - val_loss: 1.1584 - val_accuracy: 0.4444\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.1467 - accuracy: 0.5588 - val_loss: 1.1185 - val_accuracy: 0.5556\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.1327 - accuracy: 0.5824 - val_loss: 1.1464 - val_accuracy: 0.4444\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 1.1269 - accuracy: 0.5882 - val_loss: 1.1243 - val_accuracy: 0.4444\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.1127 - accuracy: 0.5794 - val_loss: 1.1309 - val_accuracy: 0.4444\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 1.1020 - accuracy: 0.5853 - val_loss: 1.1007 - val_accuracy: 0.4921\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.0922 - accuracy: 0.6059 - val_loss: 1.0670 - val_accuracy: 0.6190\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.0887 - accuracy: 0.5794 - val_loss: 1.1096 - val_accuracy: 0.4444\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.0820 - accuracy: 0.6029 - val_loss: 1.0880 - val_accuracy: 0.4603\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.0760 - accuracy: 0.5647 - val_loss: 1.0628 - val_accuracy: 0.5397\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 1.0701 - accuracy: 0.5882 - val_loss: 1.0719 - val_accuracy: 0.4444\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.0604 - accuracy: 0.5853 - val_loss: 1.0649 - val_accuracy: 0.4444\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 1.0547 - accuracy: 0.5912 - val_loss: 1.0484 - val_accuracy: 0.5079\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.0445 - accuracy: 0.5941 - val_loss: 1.0540 - val_accuracy: 0.4603\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.0393 - accuracy: 0.6059 - val_loss: 1.0355 - val_accuracy: 0.4762\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 1.0376 - accuracy: 0.5588 - val_loss: 1.0207 - val_accuracy: 0.5397\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 1.0263 - accuracy: 0.5912 - val_loss: 1.0298 - val_accuracy: 0.4921\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 1.0256 - accuracy: 0.5706 - val_loss: 1.0041 - val_accuracy: 0.5556\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.0186 - accuracy: 0.5794 - val_loss: 1.0313 - val_accuracy: 0.4444\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.0182 - accuracy: 0.5794 - val_loss: 0.9996 - val_accuracy: 0.5397\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.0033 - accuracy: 0.5765 - val_loss: 0.9996 - val_accuracy: 0.5079\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 1.0035 - accuracy: 0.5765 - val_loss: 1.0108 - val_accuracy: 0.4921\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9999 - accuracy: 0.5735 - val_loss: 0.9801 - val_accuracy: 0.5556\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9923 - accuracy: 0.6353 - val_loss: 1.0368 - val_accuracy: 0.4286\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9960 - accuracy: 0.5824 - val_loss: 0.9836 - val_accuracy: 0.4921\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 0.9834 - accuracy: 0.5882 - val_loss: 0.9784 - val_accuracy: 0.4921\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9777 - accuracy: 0.5765 - val_loss: 0.9711 - val_accuracy: 0.5238\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 0.9739 - accuracy: 0.5941 - val_loss: 0.9746 - val_accuracy: 0.4762\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9771 - accuracy: 0.5853 - val_loss: 0.9760 - val_accuracy: 0.4921\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9700 - accuracy: 0.5853 - val_loss: 0.9655 - val_accuracy: 0.4921\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9710 - accuracy: 0.5588 - val_loss: 0.9561 - val_accuracy: 0.5238\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9646 - accuracy: 0.5971 - val_loss: 0.9502 - val_accuracy: 0.5397\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9559 - accuracy: 0.6147 - val_loss: 0.9584 - val_accuracy: 0.4762\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9611 - accuracy: 0.5824 - val_loss: 0.9618 - val_accuracy: 0.4762\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 0.9458 - accuracy: 0.6088 - val_loss: 0.9504 - val_accuracy: 0.4921\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 0.9433 - accuracy: 0.5794 - val_loss: 0.9542 - val_accuracy: 0.4921\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 0.9427 - accuracy: 0.5735 - val_loss: 0.9331 - val_accuracy: 0.5397\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.9347 - accuracy: 0.6088 - val_loss: 0.9255 - val_accuracy: 0.5397\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 0.9444 - accuracy: 0.5794 - val_loss: 0.9537 - val_accuracy: 0.4603\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9374 - accuracy: 0.5853 - val_loss: 0.9229 - val_accuracy: 0.5556\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 0.9326 - accuracy: 0.5824 - val_loss: 0.9331 - val_accuracy: 0.4921\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9311 - accuracy: 0.5559 - val_loss: 0.9225 - val_accuracy: 0.5079\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9249 - accuracy: 0.5853 - val_loss: 0.9422 - val_accuracy: 0.4603\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9235 - accuracy: 0.6029 - val_loss: 0.9266 - val_accuracy: 0.4921\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.9215 - accuracy: 0.5971 - val_loss: 0.9288 - val_accuracy: 0.4762\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "img_only_history = img_only_model.fit(X_train_img_only,\n",
    "                                      y_train_img_only,\n",
    "                                      validation_data=[X_test_img_only, y_test_img_only],\n",
    "                                      batch_size=16,\n",
    "                                      epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f78a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
