{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cd4adce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pydicom\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from PIL import Image\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b610084",
   "metadata": {},
   "source": [
    "## Combine pixel arrays for each patient into a single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e550c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0473/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0473/MPRAGE/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/032_S_0677/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/032_S_0677/MPRAGE/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/032_S_0677/MPRAGE/2016-07-22_09_23_31.0/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0210/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0210/MPRAGE/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0210/MPRAGE/2016-11-03_14_21_45.0/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n"
     ]
    }
   ],
   "source": [
    "pt_ids = []\n",
    "pixels = []\n",
    "\n",
    "directory_path = '/Users/johngalvin/Downloads/ADNI 2'\n",
    "\n",
    "# Iterate through level 2 subdirectories\n",
    "for level_2_foldername in os.listdir(directory_path):\n",
    "    level_2_folder_path = os.path.join(directory_path, level_2_foldername)\n",
    "    \n",
    "    if os.path.isdir(level_2_folder_path):\n",
    "        # Iterate through DICOM files in level 5 (bottom-most level) of each level 2 folder\n",
    "        for root, _, files in os.walk(level_2_folder_path):\n",
    "            for file in files:\n",
    "                try:\n",
    "                    file_path = os.path.abspath(os.path.join(root, file))\n",
    "                    \n",
    "                    # Attempt to read DICOM file\n",
    "                    dcm = pydicom.dcmread(file_path)\n",
    "                    \n",
    "                    # Check if the file has PixelData (to avoid non-image DICOM files)\n",
    "                    if hasattr(dcm, 'PixelData'):\n",
    "                        # Append both level 2 folder name and pixel array to the lists\n",
    "                        pt_ids.append(file[5:15])\n",
    "                        pixels.append(dcm.pixel_array)\n",
    "                except Exception as e:\n",
    "                    # Handle exceptions (e.g., files without 'TransferSyntaxUID')\n",
    "                    print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "mri_df = pd.DataFrame({'PTID': pt_ids, 'Pixel Array': pixels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a118db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize image arrays with Bilinear Interpolation\n",
    "resized_arrays = []\n",
    "\n",
    "for val in mri_df[\"Pixel Array\"]:\n",
    "    image = Image.fromarray(val, mode='L')\n",
    "    resized_image = image.resize((224, 224), Image.BILINEAR)\n",
    "    resized_array = np.expand_dims(np.array(resized_image, dtype=np.uint8), axis=-1) #TF expects channel dim\n",
    "    resized_arrays.append(resized_array)\n",
    "    \n",
    "mri_df[\"Pixel Array\"] = resized_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42b20d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(vectors):\n",
    "    \"\"\"Reduces dimensionality of a group of vectors\"\"\"\n",
    "    return np.max(np.stack(vectors, axis=-1), axis=-1)[:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d490797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pool each patient's pixel arrays\n",
    "pooled_data = mri_df.groupby(\"PTID\").apply(lambda x: (max_pooling(x[\"Pixel Array\"]), x[\"PTID\"].values[0]))\n",
    "pooled_vectors, ptids = zip(*pooled_data)\n",
    "pooled_vectors = np.array(pooled_vectors)\n",
    "ptids = np.array(ptids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e21bc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "ids = []\n",
    "vectors = []\n",
    "\n",
    "for i in range(len(pooled_vectors)):\n",
    "    ids.append(ptids[i])\n",
    "    vectors.append(pooled_vectors[i])\n",
    "    \n",
    "pooled_df = pd.DataFrame()\n",
    "pooled_df[\"PTID\"] = ids\n",
    "pooled_df[\"Pooled_Vector\"] = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7797f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PTID</th>\n",
       "      <th>Pooled_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002_S_0295</td>\n",
       "      <td>[[[2], [1], [1], [1], [1], [1], [0], [2], [1],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_S_0413</td>\n",
       "      <td>[[[2], [0], [1], [1], [1], [1], [0], [1], [0],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PTID                                      Pooled_Vector\n",
       "0  002_S_0295  [[[2], [1], [1], [1], [1], [1], [0], [2], [1],...\n",
       "1  002_S_0413  [[[2], [0], [1], [1], [1], [1], [0], [1], [0],..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4dd19",
   "metadata": {},
   "source": [
    "## Join image data with clinical/genetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ed76ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_hist = pd.read_csv('../data/clinical_training_data_with_medhist_famhist.csv')\n",
    "df = pd.merge(pooled_df, mf_hist, on='PTID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d77da",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4b6178aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape Pooled_Vectors\n",
    "pooled_vectors = np.array(df[\"Pooled_Vector\"].tolist())\n",
    "flattened_vectors = pooled_vectors.reshape(pooled_vectors.shape[0], -1)\n",
    "df[\"Pooled_Vector\"] = list(flattened_vectors)\n",
    "\n",
    "# Handle Nan\n",
    "df[\"Family_History_of_AD\"] = df[\"Family_History_of_AD\"].fillna(0)\n",
    "df[\"Family_History_of_Dementia\"] = df[\"Family_History_of_Dementia\"].fillna(0)\n",
    "\n",
    "# For converting categorical variables to ints\n",
    "label_encoder = LabelEncoder()\n",
    "sequences_scaler = StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Split features / target\n",
    "X = df.drop(columns=[\"AD_dx_in_5_yrs\", \"PTID\"])\n",
    "y = df[\"AD_dx_in_5_yrs\"]\n",
    "\n",
    "# Encode features\n",
    "X[\"Diagnosis_at_Baseline\"] = label_encoder.fit_transform(X[\"Diagnosis_at_Baseline\"])\n",
    "X[\"Gender\"] = label_encoder.fit_transform(X[\"Gender\"])\n",
    "X[\"Ethnicity\"] = label_encoder.fit_transform(X[\"Ethnicity\"])\n",
    "X[\"Race\"] = label_encoder.fit_transform(X[\"Race\"])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "tmp_train = sequences_scaler.fit_transform(np.vstack(X_train[\"Pooled_Vector\"].values).astype(float))\n",
    "tmp_test = sequences_scaler.transform(np.vstack(X_test[\"Pooled_Vector\"].values).astype(float))\n",
    "\n",
    "X_train = X_train.drop(columns=[\"Pooled_Vector\"])\n",
    "X_test = X_test.drop(columns=[\"Pooled_Vector\"])\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = np.concatenate((X_train, tmp_train), axis=1)\n",
    "X_test = np.concatenate((X_test, tmp_test), axis=1)\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(sampling_strategy=\"auto\")\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f97cd7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_FEATURES = X_resampled[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2b3162dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dropout=0.3, learning_rate=0.0001):\n",
    "    \n",
    "    \"\"\"Builds classification model\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    inputs = tf.keras.layers.Input(shape=(IN_FEATURES,), name=\"input_layer\") # (Batch, num_features)\n",
    "    \n",
    "    hidden_1 = tf.keras.layers.Dense(512, activation=\"relu\", name=\"hidden_1\")(inputs)\n",
    "    hidden_1 = tf.keras.layers.Dropout(dropout)(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_2\")(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dropout(dropout)(hidden_2)\n",
    "    \n",
    "    classification = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"classification_layer\")(hidden_2)\n",
    "    classification_model = tf.keras.Model(inputs=[inputs], outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "                                 metrics=\"accuracy\")\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "761a8fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "80d5427f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johngalvin/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 2s 54ms/step - loss: 3.6009 - accuracy: 0.5294\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 3.2192 - accuracy: 0.5941\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 2.9035 - accuracy: 0.6235\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 3.3154 - accuracy: 0.7000\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 1s 48ms/step - loss: 3.8960 - accuracy: 0.6382\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 2.6208 - accuracy: 0.6794\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 2.5004 - accuracy: 0.7088\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 1.9478 - accuracy: 0.6971\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 1s 48ms/step - loss: 1.8093 - accuracy: 0.7118\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 2.0154 - accuracy: 0.7029\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 2.3990 - accuracy: 0.7265\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 1.7625 - accuracy: 0.7882\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 1.7825 - accuracy: 0.7412\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 2.1107 - accuracy: 0.7676\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 1.7653 - accuracy: 0.8000\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 1.2556 - accuracy: 0.8265\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 1.0129 - accuracy: 0.8529\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 1.0793 - accuracy: 0.8382\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 0.8763 - accuracy: 0.8853\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 1s 48ms/step - loss: 0.7444 - accuracy: 0.8824\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "history = model.fit(X_resampled,\n",
    "                    y_resampled,\n",
    "                    batch_size=16,\n",
    "                    epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "82b94188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 61ms/step - loss: 1.7419 - accuracy: 0.6935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7419241666793823, 0.6935483813285828]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b7795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
