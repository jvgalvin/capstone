{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "441ca1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johngalvin/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pydicom\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from PIL import Image\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f5d3fa",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "1. Fine-tune VGG16 on OASIS dataset \n",
    "2. Use fine-tuned VGG16 to extract features from ADNI images\n",
    "3. Pool the feature vectors for each patient into a single vector\n",
    "4. Concatenate the pooled vector with the clinical/genetic data\n",
    "5. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33da297",
   "metadata": {},
   "source": [
    "## Fine-tune VGG16 on OASIS dataset\n",
    "\n",
    "Note: We must retrain a portion of the trunk since we will remove the head we attach here to generate embeddings later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4e557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 1\n",
    "IMG_SIZE = (224,224) # Expected size for VGG16\n",
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f37c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and convert images to .jpeg\n",
    "img_dir = \"/Users/johngalvin/Desktop/OASIS/0/\"\n",
    "\n",
    "for file in os.listdir(img_dir):\n",
    "    if file.endswith(\".JPG\") or file.endswith(\".jpg\"):\n",
    "        img = Image.open(img_dir + file)\n",
    "        file_name, file_ext = os.path.splitext(file)\n",
    "        new_name = file_name + \".jpeg\"\n",
    "        img.save(img_dir + new_name)\n",
    "        \n",
    "img_dir = \"/Users/johngalvin/Desktop/OASIS/1/\"\n",
    "\n",
    "for file in os.listdir(img_dir):\n",
    "    if file.endswith(\".JPG\") or file.endswith(\".jpg\"):\n",
    "        img = Image.open(img_dir + file)\n",
    "        file_name, file_ext = os.path.splitext(file)\n",
    "        new_name = file_name + \".jpeg\"\n",
    "        img.save(img_dir + new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a14f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the .JPG and.jpg files\n",
    "img_dir = \"/Users/johngalvin/Desktop/OASIS/0/\"\n",
    "for file in os.listdir(img_dir):\n",
    "    if file.endswith(\".JPG\") or file.endswith(\".jpg\"):\n",
    "        path_to_file = os.path.join(\"/Users/johngalvin/Desktop/OASIS/0/\", file)\n",
    "        os.remove(path_to_file)\n",
    "        \n",
    "img_dir = \"/Users/johngalvin/Desktop/OASIS/1/\"\n",
    "for file in os.listdir(img_dir):\n",
    "    if file.endswith(\".JPG\") or file.endswith(\".jpg\"):\n",
    "        path_to_file = os.path.join(\"/Users/johngalvin/Desktop/OASIS/1/\", file)\n",
    "        os.remove(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2585cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "arrays = []\n",
    "\n",
    "img_dir = \"/Users/johngalvin/Desktop/OASIS/0/\"\n",
    "for file in os.listdir(img_dir):\n",
    "    fpath = os.path.join(\"/Users/johngalvin/Desktop/OASIS/0/\", file)\n",
    "    img = Image.open(fpath).convert(\"L\")  # Convert the image to grayscale\n",
    "    resized_image = img.resize((224, 224), Image.BILINEAR)  # Resize the image\n",
    "    resized_array = np.expand_dims(np.array(resized_image, dtype=np.uint8), axis=-1)  # Convert to NumPy array and add channel dimension\n",
    "    targets.append(0)\n",
    "    arrays.append(resized_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f05161",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"/Users/johngalvin/Desktop/OASIS/1/\"\n",
    "for file in os.listdir(img_dir):\n",
    "    fpath = os.path.join(\"/Users/johngalvin/Desktop/OASIS/1/\", file)\n",
    "    img = Image.open(fpath).convert(\"L\")  # Convert the image to grayscale\n",
    "    resized_image = img.resize((224, 224), Image.BILINEAR)  # Resize the image\n",
    "    resized_array = np.expand_dims(np.array(resized_image, dtype=np.uint8), axis=-1)  # Convert to NumPy array and add channel dimension\n",
    "    targets.append(1)\n",
    "    arrays.append(resized_array)\n",
    "    \n",
    "X = np.array(arrays)\n",
    "y = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a223b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance positive and negative class\n",
    "y = y[-y.sum()*2:]\n",
    "X = X[-y.sum()*2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e94fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_set, X_test, y_train_set, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reduce size to fit in memory\n",
    "X_train = X_train_set[:6000]\n",
    "y_train = y_train_set[:6000]\n",
    "X_val = X_train_set[6000:7000]\n",
    "y_val = y_train_set[6000:7000]\n",
    "X_test = X_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "num_samples, height, width, channels = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(num_samples, -1)\n",
    "X_train_scaled_2d = scaler.fit_transform(X_train_reshaped)\n",
    "X_train_scaled = X_train_scaled_2d.reshape(num_samples, height, width, channels)\n",
    "\n",
    "num_samples, height, width, channels = X_val.shape\n",
    "X_val_reshaped = X_val.reshape(num_samples, -1)\n",
    "X_val_scaled_2d = scaler.transform(X_val_reshaped)\n",
    "X_val_scaled = X_val_scaled_2d.reshape(num_samples, height, width, channels)\n",
    "\n",
    "num_samples, height, width, channels = X_test.shape\n",
    "X_test_reshaped = X_test.reshape(num_samples, -1)\n",
    "X_test_scaled_2d = scaler.transform(X_test_reshaped)\n",
    "X_test_scaled = X_test_scaled_2d.reshape(num_samples, height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1f2207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add channels (VGG16 expects 3 channels)\n",
    "X_train_rgb = np.repeat(X_train_scaled, 3, axis=-1)\n",
    "X_val_rgb = np.repeat(X_val_scaled, 3, axis=-1)\n",
    "X_test_rgb = np.repeat(X_test_scaled, 3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b4a6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model():\n",
    "    \n",
    "    base_model = VGG16(include_top=False,\n",
    "                       input_shape= IMG_SIZE + (3,),\n",
    "                       weights=\"imagenet\")\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=IMG_SIZE + (3,))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(NUM_CLASSES)(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84ceab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 13:07:10.945549: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-09 13:07:10.945618: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Pro\n"
     ]
    }
   ],
   "source": [
    "base = build_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0897878a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,108,929\n",
      "Trainable params: 394,241\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbc304e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johngalvin/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 13:08:20.667055: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 135s 1s/step - loss: 0.7013 - accuracy: 0.6078 - val_loss: 0.5591 - val_accuracy: 0.7510\n",
      "Epoch 2/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4827 - accuracy: 0.7482 - val_loss: 0.4372 - val_accuracy: 0.7840\n",
      "Epoch 3/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4431 - accuracy: 0.7807 - val_loss: 0.5232 - val_accuracy: 0.6980\n",
      "Epoch 4/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4280 - accuracy: 0.7872 - val_loss: 0.4101 - val_accuracy: 0.7880\n",
      "Epoch 5/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4171 - accuracy: 0.7898 - val_loss: 0.4977 - val_accuracy: 0.6870\n",
      "Epoch 6/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4011 - accuracy: 0.8032 - val_loss: 0.3795 - val_accuracy: 0.8130\n",
      "Epoch 7/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4326 - accuracy: 0.7785 - val_loss: 0.4021 - val_accuracy: 0.8200\n",
      "Epoch 8/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4042 - accuracy: 0.8090 - val_loss: 0.3864 - val_accuracy: 0.7990\n",
      "Epoch 9/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4021 - accuracy: 0.8115 - val_loss: 0.4211 - val_accuracy: 0.7640\n",
      "Epoch 10/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4057 - accuracy: 0.8085 - val_loss: 0.4437 - val_accuracy: 0.7170\n",
      "Epoch 11/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4185 - accuracy: 0.7920 - val_loss: 0.3740 - val_accuracy: 0.8310\n",
      "Epoch 12/15\n",
      "94/94 [==============================] - 131s 1s/step - loss: 0.4001 - accuracy: 0.7955 - val_loss: 0.3737 - val_accuracy: 0.8050\n",
      "Epoch 13/15\n",
      "94/94 [==============================] - 131s 1s/step - loss: 0.3971 - accuracy: 0.7960 - val_loss: 0.4352 - val_accuracy: 0.7660\n",
      "Epoch 14/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.3797 - accuracy: 0.8058 - val_loss: 0.4141 - val_accuracy: 0.8300\n",
      "Epoch 15/15\n",
      "94/94 [==============================] - 130s 1s/step - loss: 0.4089 - accuracy: 0.7965 - val_loss: 0.3835 - val_accuracy: 0.7900\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "base_history = base.fit(X_train_rgb,\n",
    "                        y_train,\n",
    "                        validation_data=[X_val_rgb, y_val],\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d03216bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94/94 [==============================] - 147s 2s/step - loss: 0.3910 - accuracy: 0.8053 - val_loss: 0.3005 - val_accuracy: 0.8830\n",
      "Epoch 2/10\n",
      "94/94 [==============================] - 146s 2s/step - loss: 0.2805 - accuracy: 0.8708 - val_loss: 0.2324 - val_accuracy: 0.9120\n",
      "Epoch 3/10\n",
      "94/94 [==============================] - 146s 2s/step - loss: 0.2449 - accuracy: 0.8822 - val_loss: 0.1881 - val_accuracy: 0.9290\n",
      "Epoch 4/10\n",
      "94/94 [==============================] - 146s 2s/step - loss: 0.1801 - accuracy: 0.9165 - val_loss: 0.1854 - val_accuracy: 0.9200\n",
      "Epoch 5/10\n",
      "94/94 [==============================] - 144s 2s/step - loss: 0.1450 - accuracy: 0.9248 - val_loss: 0.1555 - val_accuracy: 0.9450\n",
      "Epoch 6/10\n",
      "94/94 [==============================] - 143s 2s/step - loss: 0.1135 - accuracy: 0.9337 - val_loss: 0.1337 - val_accuracy: 0.9390\n",
      "Epoch 7/10\n",
      "94/94 [==============================] - 144s 2s/step - loss: 0.0761 - accuracy: 0.9692 - val_loss: 0.1206 - val_accuracy: 0.9620\n",
      "Epoch 8/10\n",
      "94/94 [==============================] - 143s 2s/step - loss: 0.0558 - accuracy: 0.9822 - val_loss: 0.1525 - val_accuracy: 0.9390\n",
      "Epoch 9/10\n",
      "94/94 [==============================] - 143s 2s/step - loss: 0.0236 - accuracy: 0.9948 - val_loss: 0.0920 - val_accuracy: 0.9710\n",
      "Epoch 10/10\n",
      "94/94 [==============================] - 143s 2s/step - loss: 0.0123 - accuracy: 0.9972 - val_loss: 0.0865 - val_accuracy: 0.9640\n"
     ]
    }
   ],
   "source": [
    "base.trainable = True\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                  patience=3,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "base.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "base_history_2 = base.fit(X_train_rgb,\n",
    "                          y_train,\n",
    "                          validation_data=[X_val_rgb, y_val],\n",
    "                          epochs=10,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d67ee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 6s 187ms/step - loss: 0.1406 - accuracy: 0.9560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14064964652061462, 0.956000030040741]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.evaluate(X_test_rgb, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "babc6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = tf.keras.Model(base.input, base.get_layer(\"global_average_pooling2d\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0098bcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f6f803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: feature_extractor/feature_extractor_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: feature_extractor/feature_extractor_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p feature_extractor\n",
    "feature_extractor.save(\"feature_extractor/feature_extractor_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf785df",
   "metadata": {},
   "source": [
    "## Generate embeddings for ADNI images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d78cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0473/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0473/MPRAGE/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/032_S_0677/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/032_S_0677/MPRAGE/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/032_S_0677/MPRAGE/2016-07-22_09_23_31.0/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0210/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0210/MPRAGE/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error processing file /Users/johngalvin/Downloads/ADNI 2/068_S_0210/MPRAGE/2016-11-03_14_21_45.0/.DS_Store: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n"
     ]
    }
   ],
   "source": [
    "pt_ids = []\n",
    "pixels = []\n",
    "\n",
    "directory_path = '/Users/johngalvin/Downloads/ADNI 2'\n",
    "\n",
    "# Iterate through level 2 subdirectories\n",
    "for level_2_foldername in os.listdir(directory_path):\n",
    "    level_2_folder_path = os.path.join(directory_path, level_2_foldername)\n",
    "    \n",
    "    if os.path.isdir(level_2_folder_path):\n",
    "        # Iterate through DICOM files in level 5 (bottom-most level) of each level 2 folder\n",
    "        for root, _, files in os.walk(level_2_folder_path):\n",
    "            for file in files:\n",
    "                try:\n",
    "                    file_path = os.path.abspath(os.path.join(root, file))\n",
    "                    \n",
    "                    # Attempt to read DICOM file\n",
    "                    dcm = pydicom.dcmread(file_path)\n",
    "                    \n",
    "                    # Check if the file has PixelData (to avoid non-image DICOM files)\n",
    "                    if hasattr(dcm, 'PixelData'):\n",
    "                        # Append both level 2 folder name and pixel array to the lists\n",
    "                        pt_ids.append(file[5:15])\n",
    "                        pixels.append(dcm.pixel_array)\n",
    "                except Exception as e:\n",
    "                    # Handle exceptions (e.g., files without 'TransferSyntaxUID')\n",
    "                    print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "mri_df = pd.DataFrame({'PTID': pt_ids, 'Pixel Array': pixels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83119559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize image arrays with Bilinear Interpolation\n",
    "resized_arrays = []\n",
    "\n",
    "for val in mri_df[\"Pixel Array\"]:\n",
    "    image = Image.fromarray(val, mode='L')\n",
    "    resized_image = image.resize((224, 224), Image.BILINEAR)\n",
    "    resized_array = np.expand_dims(np.array(resized_image, dtype=np.uint8), axis=-1) #TF expects channel dim\n",
    "    resized_arrays.append(resized_array)\n",
    "    \n",
    "mri_df[\"Pixel Array\"] = resized_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22814860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep just the first 160 images for each patient (size/speed)\n",
    "mri_df = mri_df.groupby(\"PTID\").head(160)\n",
    "mri_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883549c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 3 channels for VGG16 (num_samples, 224, 224, 3) - after running this cell\n",
    "resized_arrays = []\n",
    "\n",
    "for i in range(len(mri_df[\"Pixel Array\"])):\n",
    "    resized_arrays.append(np.repeat(mri_df[\"Pixel Array\"][i], 3, axis=-1))\n",
    "    \n",
    "mri_df[\"Pixel Array\"] = resized_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37dfb361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in med/famhist\n",
    "mf_hist = pd.read_csv('../data/clinical_training_data_with_medhist_famhist.csv')\n",
    "\n",
    "# Handle Nan\n",
    "mf_hist[\"Family_History_of_AD\"] = mf_hist[\"Family_History_of_AD\"].fillna(0)\n",
    "mf_hist[\"Family_History_of_Dementia\"] = mf_hist[\"Family_History_of_Dementia\"].fillna(0)\n",
    "\n",
    "# For converting categorical variables to ints\n",
    "label_encoder = LabelEncoder()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Split features / target\n",
    "X = mf_hist.drop(columns=['AD_dx_in_5_yrs'])\n",
    "y = mf_hist['AD_dx_in_5_yrs']\n",
    "\n",
    "# Encode features\n",
    "X[\"Diagnosis_at_Baseline\"] = label_encoder.fit_transform(X[\"Diagnosis_at_Baseline\"])\n",
    "X[\"Gender\"] = label_encoder.fit_transform(X[\"Gender\"])\n",
    "X[\"Ethnicity\"] = label_encoder.fit_transform(X[\"Ethnicity\"])\n",
    "X[\"Race\"] = label_encoder.fit_transform(X[\"Race\"])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5205c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale pixel data\n",
    "train_unscaled_mri_df = pd.merge(mri_df, X_train[[\"PTID\"]], on=\"PTID\", how=\"inner\")\n",
    "test_unscaled_mri_df = pd.merge(mri_df, X_test[[\"PTID\"]], on=\"PTID\", how=\"inner\")\n",
    "\n",
    "train_unscaled_arrays = np.array(train_unscaled_mri_df[\"Pixel Array\"].tolist())\n",
    "test_unscaled_arrays = np.array(test_unscaled_mri_df[\"Pixel Array\"].tolist())\n",
    "\n",
    "mean = np.mean(train_unscaled_arrays, axis=(0, 1, 2))\n",
    "std = np.std(train_unscaled_arrays, axis=(0, 1, 2))\n",
    "\n",
    "train_scaled_array = (train_unscaled_arrays - mean) / std\n",
    "test_scaled_array = (test_unscaled_arrays - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da106b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder labels (order changed when forming unscaled_mri_df)\n",
    "y_train_final = []\n",
    "for val in train_unscaled_mri_df[\"PTID\"].value_counts().index.values:\n",
    "    y_train_final.append(mf_hist.loc[mf_hist[\"PTID\"] == val, \"AD_dx_in_5_yrs\"].values[0])\n",
    "y_train_final = np.array(y_train_final)\n",
    "\n",
    "y_test_final = []\n",
    "for val in test_unscaled_mri_df[\"PTID\"].value_counts().index.values:\n",
    "    y_test_final.append(mf_hist.loc[mf_hist[\"PTID\"] == val, \"AD_dx_in_5_yrs\"].values[0])\n",
    "y_test_final = np.array(y_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97cb6e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [23.54082765 23.54082765 23.54082765]\n",
      "STD: [31.03262227 31.03262227 31.03262227]\n"
     ]
    }
   ],
   "source": [
    "print (f'Mean: {mean}')\n",
    "print (f'STD: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7014d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 08:52:20.516508: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-10 08:52:20.516741: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "feature_extractor = tf.keras.models.load_model(\"feature_extractor/feature_extractor_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80c49b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings and pool\n",
    "patient_ids = []\n",
    "latent_reps = []\n",
    "\n",
    "start = 0\n",
    "end = 160\n",
    "for i in range(len(train_unscaled_mri_df[\"PTID\"].value_counts().index.values)):\n",
    "    p_id = train_unscaled_mri_df[\"PTID\"].value_counts().index.values[i]\n",
    "    patient_ids.append(p_id)\n",
    "    features = feature_extractor(train_scaled_array[start:end]).numpy() # Each patient has 160 images (160, 512)\n",
    "    latent_rep = np.mean(np.stack(features, axis=-1), axis=1) # (512,), max pooling\n",
    "    latent_reps.append(latent_rep)\n",
    "    start += 160\n",
    "    end += 160\n",
    "\n",
    "train_embeddings = pd.DataFrame()\n",
    "train_embeddings[\"PTID\"] = patient_ids\n",
    "train_embeddings[\"embedding\"] = latent_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e01ab22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings and pool\n",
    "patient_ids = []\n",
    "latent_reps = []\n",
    "\n",
    "start = 0\n",
    "end = 160\n",
    "for i in range(len(test_unscaled_mri_df[\"PTID\"].value_counts().index.values)):\n",
    "    p_id = test_unscaled_mri_df[\"PTID\"].value_counts().index.values[i]\n",
    "    patient_ids.append(p_id)\n",
    "    features = feature_extractor(test_scaled_array[start:end]).numpy() # Each patient has 160 images (160, 512)\n",
    "    latent_rep = np.mean(np.stack(features, axis=-1), axis=1) # (512,), max pooling\n",
    "    latent_reps.append(latent_rep)\n",
    "    start += 160\n",
    "    end += 160\n",
    "\n",
    "test_embeddings = pd.DataFrame()\n",
    "test_embeddings[\"PTID\"] = patient_ids\n",
    "test_embeddings[\"embedding\"] = latent_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f9e90bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join back into X_train, X_test\n",
    "X_train = pd.merge(train_embeddings, X_train, on=\"PTID\")\n",
    "X_test = pd.merge(test_embeddings, X_test, on=\"PTID\")\n",
    "\n",
    "# Scale remaining data (clinical/genetic)\n",
    "embedding_column_train = X_train[\"embedding\"]\n",
    "embedding_column_test = X_test[\"embedding\"]\n",
    "\n",
    "columns_to_scale_train = X_train.drop(columns=[\"embedding\", \"PTID\"])\n",
    "columns_to_scale_test = X_test.drop(columns=[\"embedding\", \"PTID\"])\n",
    "\n",
    "scaled_train = scaler.fit_transform(columns_to_scale_train)\n",
    "scaled_test = scaler.transform(columns_to_scale_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(data=scaled_train, columns=columns_to_scale_train.columns)\n",
    "X_test_scaled = pd.DataFrame(data=scaled_test, columns=columns_to_scale_test.columns)\n",
    "\n",
    "X_train_scaled[\"embedding\"] = embedding_column_train\n",
    "X_test_scaled[\"embedding\"] = embedding_column_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad7f6359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure for training\n",
    "vector_column = X_train_scaled[\"embedding\"].values\n",
    "other_columns = X_train_scaled.drop(columns=[\"embedding\"]).values\n",
    "X_train_final = np.hstack((other_columns, np.vstack(vector_column)))\n",
    "\n",
    "# Structure for test\n",
    "vector_column = X_test_scaled[\"embedding\"].values\n",
    "other_columns = X_test_scaled.drop(columns=[\"embedding\"]).values\n",
    "X_test_final = np.hstack((other_columns, np.vstack(vector_column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7491ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_FEATURES = X_train_final[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9880011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dropout=0.3, learning_rate=0.0001):\n",
    "    \"\"\"Builds classification model\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    inputs = tf.keras.layers.Input(shape=(IN_FEATURES,), name=\"input_layer\") # (Batch, num_features)\n",
    "    \n",
    "    hidden_1 = tf.keras.layers.Dense(512, activation=\"relu\", name=\"hidden_1\")(inputs)\n",
    "    hidden_1 = tf.keras.layers.Dropout(dropout)(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_2\")(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dropout(dropout)(hidden_2)\n",
    "    \n",
    "    classification = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"classification_layer\")(hidden_2)\n",
    "    classification_model = tf.keras.Model(inputs=[inputs], outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "                                 metrics=\"accuracy\")\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bb04648",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05da5fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johngalvin/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n",
      "2023-10-10 08:59:11.265765: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "/Users/johngalvin/miniforge3/lib/python3.9/site-packages/keras/backend.py:5676: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 35ms/step - loss: 0.6477 - accuracy: 0.6518 - val_loss: 0.5633 - val_accuracy: 0.7460\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.6195 - accuracy: 0.6883 - val_loss: 0.5605 - val_accuracy: 0.7460\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6037 - accuracy: 0.6923 - val_loss: 0.5571 - val_accuracy: 0.7460\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6032 - accuracy: 0.6883 - val_loss: 0.5456 - val_accuracy: 0.7460\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5895 - accuracy: 0.7004 - val_loss: 0.5436 - val_accuracy: 0.7460\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.5733 - accuracy: 0.6883 - val_loss: 0.5336 - val_accuracy: 0.7460\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5710 - accuracy: 0.7045 - val_loss: 0.5348 - val_accuracy: 0.7460\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5594 - accuracy: 0.6923 - val_loss: 0.5253 - val_accuracy: 0.7460\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5586 - accuracy: 0.6923 - val_loss: 0.5300 - val_accuracy: 0.7460\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5328 - accuracy: 0.7045 - val_loss: 0.5182 - val_accuracy: 0.7460\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5365 - accuracy: 0.7206 - val_loss: 0.5177 - val_accuracy: 0.7460\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5239 - accuracy: 0.7126 - val_loss: 0.5071 - val_accuracy: 0.7460\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5241 - accuracy: 0.7085 - val_loss: 0.5046 - val_accuracy: 0.7460\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5105 - accuracy: 0.7449 - val_loss: 0.4965 - val_accuracy: 0.7460\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5276 - accuracy: 0.6842 - val_loss: 0.4915 - val_accuracy: 0.7460\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5173 - accuracy: 0.7571 - val_loss: 0.4880 - val_accuracy: 0.7619\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4861 - accuracy: 0.7126 - val_loss: 0.4826 - val_accuracy: 0.7619\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4806 - accuracy: 0.7409 - val_loss: 0.4803 - val_accuracy: 0.7619\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4734 - accuracy: 0.7895 - val_loss: 0.4786 - val_accuracy: 0.7619\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4828 - accuracy: 0.7287 - val_loss: 0.4737 - val_accuracy: 0.7619\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "history = model.fit(X_train_final,\n",
    "                    y_train_final,\n",
    "                    validation_data=[X_test_final, y_test_final],\n",
    "                    batch_size=16,\n",
    "                    epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a963be",
   "metadata": {},
   "source": [
    "# What if we do image-only?\n",
    "\n",
    "Note: this model below accepts the pooled embeddings formed after VGG16 feature extraction. The only difference from above is that it does **not** concatenate each final pooled embedding with the clinical/genetic feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6c6ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_img_only = np.vstack(X_train_scaled[\"embedding\"])\n",
    "y_train_img_only = y_train_final\n",
    "\n",
    "X_test_img_only = np.vstack(X_test_scaled[\"embedding\"])\n",
    "y_test_img_only = y_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b661b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dropout=0.3, learning_rate=0.0001):\n",
    "    \"\"\"Builds classification model\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    inputs = tf.keras.layers.Input(shape=(512,), name=\"input_layer\") # (Batch, num_features)\n",
    "    \n",
    "    hidden_1 = tf.keras.layers.Dense(512, activation=\"relu\", name=\"hidden_1\")(inputs)\n",
    "    hidden_1 = tf.keras.layers.Dropout(dropout)(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_2\")(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dropout(dropout)(hidden_2)\n",
    "    \n",
    "    classification = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"classification_layer\")(hidden_2)\n",
    "    classification_model = tf.keras.Model(inputs=[inputs], outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "                                 metrics=\"accuracy\")\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5bd4deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_only_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db78324a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 4/16 [======>.......................] - ETA: 0s - loss: 0.6906 - accuracy: 0.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johngalvin/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n",
      "/Users/johngalvin/miniforge3/lib/python3.9/site-packages/keras/backend.py:5676: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 26ms/step - loss: 0.6495 - accuracy: 0.6478 - val_loss: 0.5713 - val_accuracy: 0.7460\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6561 - accuracy: 0.6883 - val_loss: 0.5703 - val_accuracy: 0.7460\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6321 - accuracy: 0.6883 - val_loss: 0.5724 - val_accuracy: 0.7460\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6241 - accuracy: 0.6883 - val_loss: 0.5705 - val_accuracy: 0.7460\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6188 - accuracy: 0.6883 - val_loss: 0.5702 - val_accuracy: 0.7460\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6143 - accuracy: 0.6883 - val_loss: 0.5715 - val_accuracy: 0.7460\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6155 - accuracy: 0.6883 - val_loss: 0.5652 - val_accuracy: 0.7460\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6085 - accuracy: 0.6802 - val_loss: 0.5706 - val_accuracy: 0.7460\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6051 - accuracy: 0.6964 - val_loss: 0.5720 - val_accuracy: 0.7460\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6180 - accuracy: 0.6842 - val_loss: 0.5664 - val_accuracy: 0.7460\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6068 - accuracy: 0.6883 - val_loss: 0.5707 - val_accuracy: 0.7460\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6248 - accuracy: 0.6842 - val_loss: 0.5762 - val_accuracy: 0.7460\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6000 - accuracy: 0.6883 - val_loss: 0.5650 - val_accuracy: 0.7460\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6064 - accuracy: 0.6923 - val_loss: 0.5711 - val_accuracy: 0.7460\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5977 - accuracy: 0.6964 - val_loss: 0.5686 - val_accuracy: 0.7460\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5950 - accuracy: 0.6964 - val_loss: 0.5718 - val_accuracy: 0.7460\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6153 - accuracy: 0.6640 - val_loss: 0.5710 - val_accuracy: 0.7460\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5950 - accuracy: 0.6923 - val_loss: 0.5633 - val_accuracy: 0.7460\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6028 - accuracy: 0.6883 - val_loss: 0.5704 - val_accuracy: 0.7460\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6033 - accuracy: 0.6883 - val_loss: 0.5751 - val_accuracy: 0.7460\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "img_only_history = img_only_model.fit(X_train_img_only,\n",
    "                                      y_train_img_only,\n",
    "                                      validation_data=[X_test_img_only, y_test_img_only],\n",
    "                                      batch_size=16,\n",
    "                                      epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ecd9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
